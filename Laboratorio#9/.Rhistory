mexico_2023 <- exportaciones_2023 %>% filter(PAIS == "MEXICO")
china_2023 <- exportaciones_2023 %>% filter(PAIS == "CHINA")
# Verificar la cantidad de observaciones
cat("Observaciones para México: ", nrow(mexico_2023), "\n")
cat("Observaciones para China: ", nrow(china_2023), "\n")
if (nrow(mexico_2023) >= 2 & nrow(china_2023) >= 2) {
resultado <- t.test(mexico_2023$EXPORTACIONES, china_2023$EXPORTACIONES)
print(resultado)
} else {
cat("No hay suficientes datos para realizar la prueba t.\n")
}
# Filtrar datos de países asiáticos (China y Japón)
asiaticos_2023 <- exportaciones_2023 %>% filter(PAIS %in% c("CHINA", "JAPON"))
# Filtrar datos de países americanos (México y Estados Unidos)
americanos_2023 <- exportaciones_2023 %>% filter(PAIS %in% c("MEXICO", "ESTADOS UNIDOS DE AMERICA"))
# Verificar las observaciones
cat("Observaciones para países asiáticos: ", nrow(asiaticos_2023), "\n")
cat("Observaciones para países americanos: ", nrow(americanos_2023), "\n")
if (nrow(asiaticos_2023) >= 2) {
resultado_asiaticos <- t.test(asiaticos_2023$EXPORTACIONES ~ asiaticos_2023$PAIS)
print(resultado_asiaticos)
} else {
cat("No hay suficientes datos para la prueba t en países asiáticos.\n")
}
if (nrow(americanos_2023) >= 2) {
resultado_americanos <- t.test(americanos_2023$EXPORTACIONES ~ americanos_2023$PAIS)
print(resultado_americanos)
} else {
cat("No hay suficientes datos para la prueba t en países americanos.\n")
}
# Análisis descriptivo por país y mes
resumen_mensual <- exportaciones_2023 %>%
group_by(PAIS, MES) %>%
summarise(Media = mean(EXPORTACIONES, na.rm = TRUE))
print(resumen_mensual)
# Gráfico de tendencias mensuales para 2023
ggplot(exportaciones_2023, aes(x = MES, y = EXPORTACIONES, color = PAIS)) +
geom_line() +
theme_minimal() +
labs(title = "Tendencia Mensual de Exportaciones en 2023", y = "Exportaciones (USD)", x = "Mes")
# Análisis descriptivo por país y mes
resumen_mensual <- exportaciones_2023 %>%
group_by(PAIS, MES) %>%
summarise(Media = mean(EXPORTACIONES, na.rm = TRUE))
print(resumen_mensual)
# Gráfico de tendencias mensuales para 2023
ggplot(exportaciones_2023, aes(x = MES, y = EXPORTACIONES, color = PAIS)) +
geom_line() +
theme_minimal() +
labs(title = "Tendencia Mensual de Exportaciones en 2023", y = "Exportaciones (USD)", x = "Mes")
# Análisis descriptivo por país
resumen_2023 <- exportaciones_2023 %>%
group_by(PAIS) %>%
summarise(
Media = mean(EXPORTACIONES, na.rm = TRUE),
Mediana = median(EXPORTACIONES, na.rm = TRUE),
Desviacion = sd(EXPORTACIONES, na.rm = TRUE),
Minimo = min(EXPORTACIONES, na.rm = TRUE),
Maximo = max(EXPORTACIONES, na.rm = TRUE)
)
# Mostrar el resumen
print(resumen_2023)
library(ggplot2)
# Gráfico de barras para 2023
ggplot(exportaciones_2023, aes(x = PAIS, y = EXPORTACIONES, fill = PAIS)) +
geom_bar(stat = "identity") +
theme_minimal() +
labs(title = "Exportaciones FOB por País en 2023", y = "Exportaciones (USD)", x = "País")
# Gráfico de líneas para cada año
exportaciones_fob_long <- exportaciones_fob %>%
pivot_longer(cols = `2016`:`2023`, names_to = "AÑO", values_to = "EXPORTACIONES")
# Gráfico de densidad por país
ggplot(exportaciones_2023, aes(x = EXPORTACIONES, fill = PAIS)) +
geom_density(alpha = 0.5) +
theme_minimal() +
labs(title = "Densidad de Exportaciones por País en 2023",
y = "Densidad", x = "Exportaciones (USD)")
# Filtrar datos de 2016 y 2023 para países asiáticos
asiaticos_2016 <- exportaciones_fob %>% filter(PAIS %in% c("CHINA", "JAPON")) %>%
select(PAIS, `2016`) %>% rename(EXPORTACIONES = `2016`)
asiaticos_2023 <- exportaciones_fob %>% filter(PAIS %in% c("CHINA", "JAPON")) %>%
select(PAIS, `2023`) %>% rename(EXPORTACIONES = `2023`)
# Prueba t entre 2016 y 2023 para países asiáticos
if (nrow(asiaticos_2016) >= 2 & nrow(asiaticos_2023) >= 2) {
resultado_asiaticos <- t.test(asiaticos_2016$EXPORTACIONES, asiaticos_2023$EXPORTACIONES)
print(resultado_asiaticos)
} else {
cat("No hay suficientes datos para la prueba t en países asiáticos.\n")
}
# Prueba de varianza entre México y China en 2023
var_test <- var.test(mexico_2023$EXPORTACIONES, china_2023$EXPORTACIONES)
print(var_test)
plumber::plumb(file='Documents/GitHub/Proyecto-DataProduct/Proyecto/api.R')$run()
plumb(file='Documents/GitHub/Proyecto-DataProduct/Proyecto/api.R')$run()
plumb(file='Documents/GitHub/Proyecto-DataProduct/Proyecto/api.R')$run()
plumb(file='Documents/GitHub/Proyecto-DataProduct/Proyecto/api.R')$run()
plumb(file='Documents/GitHub/Proyecto-DataProduct/Proyecto/api.R')$run()
setwd("~/Documents/GitHub/Data-Wrangling/Laboratorio #7")
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(stopwords)
install.packages("stopwords")
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(stopwords)
library(wordcloud)
# Carga de datos de productos y limpieza de texto
product_reviews <- read_csv("Health_and_Personal_Care.csv")
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(stopwords)
library(wordcloud)
# Carga de datos de productos y limpieza de texto
product_reviews <- read_csv("Health_and_Personal_Care.csv")
product_reviews$text <- str_replace_all(product_reviews$text, '\\"', '')
head(product_reviews)
product_metadata <- read_csv("Health_and_Personal_Care_metadata.csv")
head(product_metadata)
# Definición de palabras clave y búsqueda de reseñas positivas
keywords <- c("love", "recommend", "enjoy")
positive_pattern <- paste(keywords, collapse = '|')
positive_reviews <- product_reviews %>%
filter(str_detect(text, positive_pattern)) %>%
distinct(product_id) %>%
summarise(total_positive_products = n())
positive_reviews
# Filtrado de productos y agrupación por tienda
filtered_reviews <- product_reviews %>%
filter(str_detect(text, positive_pattern)) %>%
distinct(product_id, parent_id)
top_stores <- filtered_reviews %>%
inner_join(product_metadata, by = "parent_id") %>%
filter(!is.na(store)) %>%
count(store, name = "total_products") %>%
arrange(desc(total_products)) %>%
top_n(5) %>%
select(store)
top_stores
# Extracción de palabras sin stopwords para nube de palabras
positive_texts <- product_reviews %>%
filter(str_detect(text, positive_pattern)) %>%
distinct(product_id, text) %>%
select(text)
# Generación de lista de palabras sin stopwords
stop_words_list <- c(stopwords("en"), stopwords("es"))
word_list <- str_split(positive_texts$text[1:100], boundary("word")) %>% unlist()
filtered_words <- tibble(word = word_list) %>%
filter(!word %in% stop_words_list) %>%
count(word, name = "frequency")
wordcloud(filtered_words$word, filtered_words$frequency)
# Análisis de palabras para tiendas seleccionadas
selected_stores <- c(top_stores)
store_parents <- product_metadata %>%
filter(store %in% selected_stores) %>%
select(parent_id)
store_reviews <- product_reviews %>%
inner_join(store_parents, by = "parent_id") %>%
select(text)
word_list_store <- str_split(store_reviews$text[1:100], boundary("word")) %>% unlist()
filtered_words_store <- tibble(word = word_list_store) %>%
filter(!word %in% stop_words_list) %>%
count(word, name = "frequency")
wordcloud(filtered_words_store$word, filtered_words_store$frequency)
# Análisis de palabras para tiendas seleccionadas
selected_stores <- c(top_stores)
store_parents <- product_metadata %>%
filter(store %in% selected_stores) %>%
select(parent_id)
store_reviews <- product_reviews %>%
inner_join(store_parents, by = "parent_id") %>%
select(text)
word_list_store <- str_split(store_reviews$text[1:100], boundary("word")) %>% unlist()
filtered_words_store <- tibble(word = word_list_store) %>%
filter(!word %in% stop_words_list) %>%
count(word, name = "frequency")
wordcloud(filtered_words_store$word, filtered_words_store$frequency)
# Análisis de palabras para las principales tiendas seleccionadas
selected_stores <- top_stores$store
store_parents <- product_metadata %>%
filter(store %in% selected_stores) %>%
select(parent_id)
store_reviews <- product_reviews %>%
inner_join(store_parents, by = "parent_id") %>%
select(text)
word_list_store <- store_reviews$text %>%
head(100) %>%  # Selecciona las primeras 100 reseñas
str_split(boundary("word")) %>%
unlist()
filtered_words_store <- tibble(word = word_list_store) %>%
filter(!word %in% stop_words_list) %>%
count(word, name = "frequency")
# Genera la nube de palabras
wordcloud(filtered_words_store$word, filtered_words_store$frequency)
# Análisis general de palabras en reseñas
all_words <- str_split(product_reviews$text, boundary("word")) %>% unlist()
filtered_words_general <- tibble(word = all_words) %>%
filter(!word %in% stop_words_list) %>%
count(word, name = "frequency") %>%
arrange(desc(frequency))
top_25_words <- filtered_words_general %>%
top_n(25, frequency)
top_25_words
View(product_reviews)
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(stopwords)
library(wordcloud)
product_reviews <- read_csv("Health_and_Personal_Care.csv")
product_reviews$text <- str_replace_all(product_reviews$text, '\\"', '')
product_metadata <- read_csv("Health_and_Personal_Care_metadata.csv")
View(product_reviews)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(stringr)
library(tidytext)
library(readr)
library(tidyverse)
library(stringr)
library(tidytext)
library(lubridate)
library(stopwords)
library(wordcloud)
df <- read_csv("Health_and_Personal_Care.csv")
words <- c(
"love"
,"recommend"
,"enjoy"
)
positive_words <- paste(words, collapse = '|')
positive <- df %>%
filter(str_detect(string = df$text,pattern = positive_words)) %>%
distinct(product_id)%>%
count(name = "products_with_positive_review")
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(stopwords)
library(wordcloud)
product_reviews <- read_csv("Health_and_Personal_Care.csv")
product_reviews$text <- str_replace_all(product_reviews$text, '\\"', '')
product_metadata <- read_csv("Health_and_Personal_Care_metadata.csv")
words <- c(
"love"
,"recommend"
,"enjoy"
)
positive_words <- paste(words, collapse = '|')
positive <- df %>%
filter(str_detect(string = df$text,pattern = positive_words)) %>%
distinct(product_id)%>%
count(name = "products_with_positive_review")
words <- c(
"love"
,"recommend"
,"enjoy"
)
positive_words <- paste(words, collapse = '|')
positive <- product_reviews %>%
filter(str_detect(string = df$text,pattern = positive_words)) %>%
distinct(product_id)%>%
count(name = "products_with_positive_review")
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(stopwords)
library(wordcloud)
df <- read_csv("Health_and_Personal_Care.csv")
df$text <- str_replace_all(df$text, pattern = '\\"', replacement = '')
df %>% head()
meta <- read_csv("Health_and_Personal_Care_metadata.csv")
meta %>% head()
words <- c(
"love"
,"recommend"
,"enjoy"
)
positive_words <- paste(words, collapse = '|')
positive <- df %>%
filter(str_detect(string = df$text,pattern = positive_words)) %>%
distinct(product_id)%>%
count(name = "products_with_positive_review")
positive
positive_texts <- product_reviews %>%
filter(str_detect(text, positive_pattern)) %>%
distinct(product_id, text) %>%
select(text)
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(stopwords)
library(wordcloud)
product_reviews <- read_csv("Health_and_Personal_Care.csv")
product_reviews$text <- str_replace_all(product_reviews$text, '\\"', '')
product_metadata <- read_csv("Health_and_Personal_Care_metadata.csv")
setwd("~/Documents/GitHub/Data-Wrangling/Laboratorio#9")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load required libraries
library(tidyverse)
library(mice)
install.packages("mice")
install.packages("VIM")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load required libraries
library(tidyverse)
library(mice)
library(VIM)
library(caret)
library(scales)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load required libraries
library(tidyverse)
library(mice)
library(VIM)
library(caret)
library(scales)
# Load required libraries
library(tidyverse)
library(mice)
library(VIM)
library(caret)
library(scales)
# Load required libraries
library(tidyverse)
library(mice)
library(VIM)
library(caret)
library(scales)
# Load required libraries
library(tidyverse)
library(mice)
library(VIM)
library(caret)
library(scales)
# Load required libraries
library(tidyverse)
library(mice)
library(VIM)
library(caret)
library(scales)
# Cargar los datos
titanic_md <- read.csv("titanic_MD.csv")
titanic_complete <- read.csv("titanic.csv")
# Función para calcular el porcentaje de missing values
missing_report <- function(data) {
missing <- sapply(data, function(x) sum(is.na(x) | x == "?" | x == ""))
missing_percent <- round(missing/nrow(data)*100, 2)
report <- data.frame(
Variable = names(missing),
Missing_Count = missing,
Missing_Percent = missing_percent
)
return(report[report$Missing_Count > 0, ])
}
# Generar reporte
missing_report(titanic_md)
# Identificar filas completas
complete_rows <- complete.cases(titanic_md)
cat("Filas completas:", sum(complete_rows), "\n")
cat("Porcentaje de filas completas:", round(mean(complete_rows)*100, 2), "%\n")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Cargar librerías
library(dplyr)
library(tidyr)
library(ggplot2)
# Cargar librerías
library(dplyr)
library(tidyr)
library(ggplot2)
# Cargar datos
titanic_md <- read.csv("titanic_MD.csv", na.strings = c("", "?", "NA"))
titanic_complete <- read.csv("titanic.csv")
# Calcular missing values
missing_summary <- data.frame(
Variable = names(titanic_md),
Missing_Count = colSums(is.na(titanic_md)),
Missing_Percent = round(colSums(is.na(titanic_md))/nrow(titanic_md)*100, 2)
)
# Mostrar solo variables con missing values
missing_summary[missing_summary$Missing_Count > 0, ]
methods_df <- data.frame(
Variable = c("Sex", "Age", "SibSp", "Parch"),
Método = c("Moda", "Random Forest", "Mediana", "Mediana"),
Justificación = c(
"Variable categórica binaria",
"Relaciones complejas con otras variables",
"Variable numérica discreta",
"Variable numérica discreta"
)
)
print(methods_df)
n_complete <- sum(complete.cases(titanic_md))
n_incomplete <- sum(!complete.cases(titanic_md))
cat("Filas completas:", n_complete, "\n")
cat("Filas incompletas:", n_incomplete, "\n")
cat("Porcentaje de completitud:", round(n_complete/nrow(titanic_md)*100, 2), "%")
setwd("~/Documents/GitHub/Data-Wrangling/Laboratorio#9")
# Cargar librerías
library(dplyr)
library(tidyr)
library(ggplot2)
# Cargar datos
titanic_md <- read.csv("titanic_MD.csv", na.strings = c("", "?", "NA"))
titanic_complete <- read.csv("titanic.csv")
# Calcular missing values
missing_summary <- data.frame(
Variable = names(titanic_md),
Missing_Count = colSums(is.na(titanic_md)),
Missing_Percent = round(colSums(is.na(titanic_md))/nrow(titanic_md)*100, 2)
)
# Mostrar solo variables con missing values
missing_summary[missing_summary$Missing_Count > 0, ]
methods_df <- data.frame(
Variable = c("Sex", "Age", "SibSp", "Parch"),
Método = c("Moda", "Random Forest", "Mediana", "Mediana"),
Justificación = c(
"Variable categórica binaria",
"Relaciones complejas con otras variables",
"Variable numérica discreta",
"Variable numérica discreta"
)
)
print(methods_df)
n_complete <- sum(complete.cases(titanic_md))
n_incomplete <- sum(!complete.cases(titanic_md))
cat("Filas completas:", n_complete, "\n")
cat("Filas incompletas:", n_incomplete, "\n")
cat("Porcentaje de completitud:", round(n_complete/nrow(titanic_md)*100, 2), "%")
titanic_imputed <- titanic_md
# Imputación por media/moda/mediana
titanic_imputed$Sex[is.na(titanic_imputed$Sex)] <- names(sort(table(titanic_md$Sex), decreasing = TRUE))[1]
titanic_imputed$Age[is.na(titanic_imputed$Age)] <- mean(titanic_md$Age, na.rm = TRUE)
titanic_imputed$SibSp[is.na(titanic_imputed$SibSp)] <- median(titanic_md$SibSp, na.rm = TRUE)
titanic_imputed$Parch[is.na(titanic_imputed$Parch)] <- median(titanic_md$Parch, na.rm = TRUE)
# Resumen de la imputación
summary(titanic_imputed[c("Age", "SibSp", "Parch")])
# Preparar datos para regresión
titanic_reg <- titanic_md
titanic_reg$Sex <- as.factor(titanic_reg$Sex)
titanic_reg$Sex <- as.numeric(titanic_reg$Sex)
# Modelo para Age
age_model <- lm(Age ~ Pclass + Sex + Fare,
data = titanic_reg[!is.na(titanic_reg$Age), ])
# Imputar valores faltantes de Age
titanic_reg$Age[is.na(titanic_reg$Age)] <- predict(age_model,
newdata = titanic_reg[is.na(titanic_reg$Age), ])
# Función para detectar outliers usando desviación estándar
detect_outliers <- function(x, n_sd = 2) {
mean_x <- mean(x, na.rm = TRUE)
sd_x <- sd(x, na.rm = TRUE)
x < (mean_x - n_sd * sd_x) | x > (mean_x + n_sd * sd_x)
}
# Detectar outliers en Age
age_outliers <- detect_outliers(titanic_complete$Age)
cat("Número de outliers en Age:", sum(age_outliers, na.rm = TRUE))
# Función para calcular RMSE
rmse <- function(pred, actual) {
sqrt(mean((pred - actual)^2, na.rm = TRUE))
}
# Comparar métodos para Age
age_comparison <- data.frame(
Método = c("Imputación básica", "Regresión lineal"),
RMSE = c(
rmse(titanic_imputed$Age, titanic_complete$Age),
rmse(titanic_reg$Age, titanic_complete$Age)
)
)
print(age_comparison)
# Seleccionar columnas numéricas
numeric_cols <- c("Age", "Fare")
# Standardization
titanic_std <- titanic_imputed
titanic_std[numeric_cols] <- scale(titanic_imputed[numeric_cols])
# Min-Max Scaling
titanic_minmax <- titanic_imputed
titanic_minmax[numeric_cols] <- apply(titanic_imputed[numeric_cols], 2, function(x) {
(x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
})
# MaxAbs Scaling
titanic_maxabs <- titanic_imputed
titanic_maxabs[numeric_cols] <- apply(titanic_imputed[numeric_cols], 2, function(x) {
x / max(abs(x), na.rm = TRUE)
})
# Función para obtener estadísticos resumidos
get_stats <- function(data, col) {
c(
Media = mean(data[[col]], na.rm = TRUE),
DE = sd(data[[col]], na.rm = TRUE),
Min = min(data[[col]], na.rm = TRUE),
Max = max(data[[col]], na.rm = TRUE),
Mediana = median(data[[col]], na.rm = TRUE)
)
}
# Comparar estadísticos para Age
age_stats <- rbind(
Original = get_stats(titanic_complete, "Age"),
Estandarizado = get_stats(titanic_std, "Age"),
MinMax = get_stats(titanic_minmax, "Age"),
MaxAbs = get_stats(titanic_maxabs, "Age")
)
print(round(age_stats, 3))
