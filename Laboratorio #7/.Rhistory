# Cálculo del estadístico z
z <- (p_A - p_B) / sqrt(p_comb * (1 - p_comb) * (1/n_A + 1/n_B))
# p-valor
p_value <- 1 - pnorm(z)
# Resultados
z
p_value
≈# Datos
# Datos
p_A <- 0.90
p_B <- 0.85
n_A <- 50
n_B <- 45
# Éxitos en cada grupo
x_A <- p_A * n_A
x_B <- p_B * n_B
# Proporción combinada
p_comb <- (x_A + x_B) / (n_A + n_B)
# Cálculo del estadístico z
z <- (p_A - p_B) / sqrt(p_comb * (1 - p_comb) * (1/n_A + 1/n_B))
# p-valor
p_value <- 1 - pnorm(z)
# Intervalo de confianza para la diferencia de proporciones
conf_level <- 0.95
se_diff <- sqrt(p_comb * (1 - p_comb) * (1/n_A + 1/n_B))
margin_error <- qnorm(conf_level + (1 - conf_level) / 2) * se_diff
lower_bound <- (p_A - p_B) - margin_error
upper_bound <- (p_A - p_B) + margin_error
# Resultados
z
p_value
c(lower_bound, upper_bound)
# Problema 3
# Datos de tiempo de secado para la pintura Marca A
marca_A <- c(4.6, 6.1, 2.8, 3.8, 3.1, 3.7, 6.1, 4.7, 4.2, 4.0, 5.1, 6.6, 6.0, 3.3, 4.1)
# Datos de tiempo de secado para la pintura Marca B
marca_B <- c(3.4, 2.5, 4.8, 2.9, 3.6, 2.8, 3.3, 5.6, 3.7, 2.8, 4.4, 4.0, 5.2, 3.0, 4.8)
# Realizar la prueba t para muestras independientes
t_test_result <- t.test(marca_A, marca_B, alternative = "greater", var.equal = FALSE)
# Mostrar el resultado de la prueba t
print(t_test_result)
# Problema 4
# Datos de ejemplo
diametros <- c(10.05, 9.98, 10.10, 9.95, 10.02, 9.99, 10.04, 10.01, 9.96, 10.08)
# Prueba t para la media
t_test_result <- t.test(diametros, mu = 10)
# Mostrar resultados
print(t_test_result)
# Problema 4
# Datos de ejemplo
diametros <- c(10.05, 9.98, 10.10, 9.95, 10.02, 9.99, 10.04, 10.01, 9.96, 10.08)
# Prueba t para la media
t_test_result <- t.test(diametros, mu = 10)
# Mostrar resultados
print(t_test_result)
# ---- Cargar Librerias ----
library(tidyverse)
library(ggplot2)
library(openxlsx)
#install.packages("scorecard")
library(scorecard)
# ---- Cargar Data Set German Credit ----
df <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data")
colnames(df) <- c("chk_acct", "duration", "credit_his", "purpose",
"amount", "saving_acct", "present_emp", "installment_rate", "sex", "other_debtor",
"present_resid", "property", "age", "other_install", "housing", "n_credits",
"job", "n_people", "telephone", "foreign", "response")
df$response[df$response==2] <- 0
df %>% summary()
# ---- Tablas de Frecuencia por Edad ----
df.age <- df %>% select(age, response) %>%
group_by(age) %>% summarise(
total_credit = n(),
bad_credit = sum(response),
bad_rate = mean(response)
)
df.age %>% View()
# ---- Graficando La Edad por la Tasa de Malos ----
## Graficando el bad rate
ggplot(df.age, aes(x = age, y = bad_rate)) +
geom_line() +
labs(title = "Tasa de Malos por Edad",
x = "Edad",
y = "Tasa de Malos") +
theme_minimal()
## Graficando el bad rate y la frecuencia de créditos
ggplot(df.age, aes(x = age)) +
# Barras para total_credit
geom_bar(aes(y = total_credit), stat = "identity", fill = "skyblue", alpha = 0.7) +
# Línea para bad_rate
geom_line(aes(y = bad_rate * max(total_credit)), color = "red", size = 1) +
# Añadir un segundo eje y para bad_rate
scale_y_continuous(
name = "Total de Créditos",
sec.axis = sec_axis(~ . / max(df.age$total_credit), name = "Tasa de Malos (Bad Rate)")
) +
labs(title = "Tasa de Malos de la Variable Age",
x = "Edad") +
theme_minimal()
# ---- Binning con Dplyr ----
df.age_bin <- df %>% select(age, response) %>%
mutate(age_bin = ntile(age, 5)) %>%
group_by(age_bin) %>% summarise(
bad_credit = sum(response),
good_credit = sum(1 - response),
total_credit = n(),
bad_rate = bad_credit / total_credit,
min_age = min(age),
max_age = max(age)
)
df.age_bin
ggplot(df.age_bin, aes(x = age_bin)) +
# Barras para total_credit
geom_bar(aes(y = total_credit), stat = "identity", fill = "skyblue", alpha = 0.7) +
# Línea para bad_rate
geom_line(aes(y = bad_rate * max(total_credit)), color = "red", size = 1) +
# Añadir un segundo eje y para bad_rate
scale_y_continuous(
name = "Total de Créditos",
sec.axis = sec_axis(~ . / max(df.age_bin$total_credit), name = "Tasa de Malos (Bad Rate)")
) +
labs(title = "Tasa de Malos de los Bines de la Variable Age",
x = "Bines de Edad") +
theme_minimal()
# ---- Calculando el WoE y el Information Value con Dplyr ----
df.age_bin_woe <- df.age_bin %>%
mutate(
pct_good = good_credit / sum(good_credit),
pct_bad = bad_credit / sum(bad_credit),
woe = log(pct_good / pct_bad),
iv_bin = (pct_good - pct_bad) * woe,
iv_var = sum(iv_bin)
)
df.age_bin_woe
# ---- Binning con paquete Scorecard ----
bins <- woebin(df, y = "response", x = c("age"))
bins$age
df_binned <- woebin_ply(df, bins)
head(df_binned)
plot <- woebin_plot(bins)
plot[[1]]
# ---- Ejercicio - binning con dplyr y la variable amount ----
df.amount_bin <- df %>% select(amount, response) %>%
mutate(amount_bin = ntile(amount, 5)) %>%
group_by(amount_bin) %>% summarise(
bad_credit = sum(response),
good_credit = sum(1 - response),
total_credit = n(),
bad_rate = bad_credit / total_credit
)
df.amount_bin
ggplot(df.amount_bin, aes(x = amount_bin, y = bad_rate)) +
geom_line() +
labs(title = "Tasa de Malos por los bines de Amount",
x = "Amount bin",
y = "Bad Rate") +
theme_minimal()
ggplot(df.amount_bin, aes(x = amount_bin)) +
# Barras para total_credit
geom_bar(aes(y = total_credit), stat = "identity", fill = "skyblue", alpha = 0.7) +
# Línea para bad_rate
geom_line(aes(y = bad_rate * max(total_credit)), color = "red", size = 1) +
# Añadir un segundo eje y para bad_rate
scale_y_continuous(
name = "Total de Créditos",
sec.axis = sec_axis(~ . / max(df.amount_bin$total_credit), name = "Tasa de Malos (Bad Rate)")
) +
labs(title = "Tasa de Malos por los bines de Amount",
x = "Bines de Amount") +
theme_minimal()
# ---- Mismo ejercicio con amount con el paquete Scorecard ----
bins <- woebin(df, y = "response", x = c("age", "n_credits", "amount"))
bins$amount
df_binned <- woebin_ply(df, bins)
head(df_binned)
plot <- woebin_plot(bins)
plot[[3]]
# ---- Calculando el WoE y el Information Value con Dplyr ----
df.age_bin_woe <- df.age_bin %>%
mutate(
pct_good = good_credit / sum(good_credit),
pct_bad = bad_credit / sum(bad_credit),
woe = log(pct_good / pct_bad),
iv_bin = (pct_good - pct_bad) * woe,
iv_var = sum(iv_bin)
)
df.age_bin_woe
# ---- Calculando el WoE y el Information Value con Dplyr ----
df.age_bin_woe <- df.age_bin %>%
mutate(
pct_good = good_credit / sum(good_credit),
pct_bad = bad_credit / sum(bad_credit),
woe = log(pct_good / pct_bad),
iv_bin = (pct_good - pct_bad) * woe,
iv_var = sum(iv_bin)
)
df.age_bin_woe
# ---- Binning con paquete Scorecard ----
bins <- woebin(df, y = "response", x = c("age"))
bins$age
df_binned <- woebin_ply(df, bins)
head(df_binned)
plot <- woebin_plot(bins)
plot[[1]]
View(df.age_bin_woe)
View(df_binned)
View(df.amount_bin)
shiny::runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
shiny::runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
runApp('Desktop/Data Producto/Laboratorio #2/Lab2_Shiny_Web_App')
plumber::plumb(file='Desktop/Data Producto/Laboratorio #4/weather_api.R')$run()
library(readxl)
exportaciones_fob <- read_excel("Desktop/lab Javier/exportaciones_fob.xlsx")
View(exportaciones_fob)
library(readxl)
exportaciones_fob <- read_excel("Desktop/lab Javier/exportaciones_fob.xlsx")
View(exportaciones_fob)```
library(readxl)
exportaciones_fob <- read_excel("Desktop/lab Javier/exportaciones_fob.xlsx")
View(exportaciones_fob)
# Ver las primeras filas del dataset
head(exportaciones_fob)
# Ver resumen general del dataset
summary(exportaciones_fob)
# Ver estructura de las columnas
str(exportaciones_fob)
# Filtrar datos para el año 2023
exportaciones_2023 <- exportaciones_fob %>% filter(AÑO == 2023)
library(dplyr)
# Filtrar datos para el año 2023
exportaciones_2023 <- exportaciones_fob %>% filter(AÑO == 2023)
library(dplyr)
# Filtrar datos para el año 2023
exportaciones_2023 <- exportaciones_fob %>% filter(AÑO == 2023)
View(exportaciones_fob)
library(dplyr)
# Filtrar datos para el año 2023
exportaciones_2023 <- exportaciones_fob %>% filter(2023)
library(dplyr)
# Filtrar datos para el año 2023
exportaciones_2023 <- exportaciones_fob %>% filter by colnames(2023)
View(exportaciones_fob)
library(dplyr)
# Filtrar datos para el año 2023
exportaciones_2023 <- exportaciones_fob[10]
# Verificar los datos filtrados
head(exportaciones_2023)
# Cargar dplyr para manipulación de datos
library(dplyr)
# Análisis descriptivo por país
resumen_2023 <- exportaciones_2023 %>%
group_by(PAIS) %>%
summarise(
Media = mean(EXPORTACIONES, na.rm = TRUE),
Mediana = median(EXPORTACIONES, na.rm = TRUE),
Desviacion = sd(EXPORTACIONES, na.rm = TRUE),
Minimo = min(EXPORTACIONES, na.rm = TRUE),
Maximo = max(EXPORTACIONES, na.rm = TRUE)
)
View(exportaciones_fob)
# Seleccionar los datos de exportaciones para 2023
exportaciones_2023 <- exportaciones_fob %>%
select(PAIS, MES, `2023`)
# Renombrar la columna para que sea más fácil de manejar
exportaciones_2023 <- exportaciones_2023 %>%
rename(EXPORTACIONES = `2023`)
# Verificar los datos seleccionados
head(exportaciones_2023)
View(exportaciones_fob)
View(exportaciones_2023)
# Análisis descriptivo por país
resumen_2023 <- exportaciones_2023 %>%
group_by(PAIS) %>%
summarise(
Media = mean(EXPORTACIONES, na.rm = TRUE),
Mediana = median(EXPORTACIONES, na.rm = TRUE),
Desviacion = sd(EXPORTACIONES, na.rm = TRUE),
Minimo = min(EXPORTACIONES, na.rm = TRUE),
Maximo = max(EXPORTACIONES, na.rm = TRUE)
)
# Mostrar el resumen
print(resumen_2023)
library(ggplot2)
# Gráfico de barras para 2023
ggplot(exportaciones_2023, aes(x = PAIS, y = EXPORTACIONES, fill = PAIS)) +
geom_bar(stat = "identity") +
theme_minimal() +
labs(title = "Exportaciones FOB por País en 2023", y = "Exportaciones (USD)", x = "País")
ggplot(exportaciones_2023, aes(x = PAIS, y = EXPORTACIONES, fill = PAIS)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Distribución de Exportaciones por País en 2023", y = "Exportaciones (USD)", x = "País")
# Filtrar datos para México y China
mexico_2023 <- exportaciones_2023 %>% filter(PAIS == "México")
china_2023 <- exportaciones_2023 %>% filter(PAIS == "China")
# Prueba t para comparar los promedios
t.test(mexico_2023$EXPORTACIONES, china_2023$EXPORTACIONES)
# Verificar cuántas observaciones hay para México y China en 2023
mexico_2023 <- exportaciones_2023 %>% filter(PAIS == "México")
china_2023 <- exportaciones_2023 %>% filter(PAIS == "China")
# Verificar las dimensiones de cada conjunto de datos
cat("Observaciones para México: ", nrow(mexico_2023), "\n")
cat("Observaciones para China: ", nrow(china_2023), "\n")
# Ver las primeras filas de cada conjunto
head(mexico_2023)
head(china_2023)
# Listar los valores únicos en la columna PAIS
unique(exportaciones_2023$PAIS)
# Filtrar las exportaciones de 2023 para México y China
mexico_2023 <- exportaciones_2023 %>% filter(PAIS == "MEXICO")
china_2023 <- exportaciones_2023 %>% filter(PAIS == "CHINA")
# Verificar la cantidad de observaciones
cat("Observaciones para México: ", nrow(mexico_2023), "\n")
cat("Observaciones para China: ", nrow(china_2023), "\n")
# Filtrar las exportaciones de 2023 para México y China
mexico_2023 <- exportaciones_2023 %>% filter(PAIS == "MEXICO")
china_2023 <- exportaciones_2023 %>% filter(PAIS == "CHINA")
# Verificar la cantidad de observaciones
cat("Observaciones para México: ", nrow(mexico_2023), "\n")
cat("Observaciones para China: ", nrow(china_2023), "\n")
if (nrow(mexico_2023) >= 2 & nrow(china_2023) >= 2) {
resultado <- t.test(mexico_2023$EXPORTACIONES, china_2023$EXPORTACIONES)
print(resultado)
} else {
cat("No hay suficientes datos para realizar la prueba t.\n")
}
# Filtrar datos de países asiáticos (China y Japón)
asiaticos_2023 <- exportaciones_2023 %>% filter(PAIS %in% c("CHINA", "JAPON"))
# Filtrar datos de países americanos (México y Estados Unidos)
americanos_2023 <- exportaciones_2023 %>% filter(PAIS %in% c("MEXICO", "ESTADOS UNIDOS DE AMERICA"))
# Verificar las observaciones
cat("Observaciones para países asiáticos: ", nrow(asiaticos_2023), "\n")
cat("Observaciones para países americanos: ", nrow(americanos_2023), "\n")
if (nrow(asiaticos_2023) >= 2) {
resultado_asiaticos <- t.test(asiaticos_2023$EXPORTACIONES ~ asiaticos_2023$PAIS)
print(resultado_asiaticos)
} else {
cat("No hay suficientes datos para la prueba t en países asiáticos.\n")
}
if (nrow(americanos_2023) >= 2) {
resultado_americanos <- t.test(americanos_2023$EXPORTACIONES ~ americanos_2023$PAIS)
print(resultado_americanos)
} else {
cat("No hay suficientes datos para la prueba t en países americanos.\n")
}
# Análisis descriptivo por país y mes
resumen_mensual <- exportaciones_2023 %>%
group_by(PAIS, MES) %>%
summarise(Media = mean(EXPORTACIONES, na.rm = TRUE))
print(resumen_mensual)
# Gráfico de tendencias mensuales para 2023
ggplot(exportaciones_2023, aes(x = MES, y = EXPORTACIONES, color = PAIS)) +
geom_line() +
theme_minimal() +
labs(title = "Tendencia Mensual de Exportaciones en 2023", y = "Exportaciones (USD)", x = "Mes")
# Análisis descriptivo por país y mes
resumen_mensual <- exportaciones_2023 %>%
group_by(PAIS, MES) %>%
summarise(Media = mean(EXPORTACIONES, na.rm = TRUE))
print(resumen_mensual)
# Gráfico de tendencias mensuales para 2023
ggplot(exportaciones_2023, aes(x = MES, y = EXPORTACIONES, color = PAIS)) +
geom_line() +
theme_minimal() +
labs(title = "Tendencia Mensual de Exportaciones en 2023", y = "Exportaciones (USD)", x = "Mes")
# Análisis descriptivo por país
resumen_2023 <- exportaciones_2023 %>%
group_by(PAIS) %>%
summarise(
Media = mean(EXPORTACIONES, na.rm = TRUE),
Mediana = median(EXPORTACIONES, na.rm = TRUE),
Desviacion = sd(EXPORTACIONES, na.rm = TRUE),
Minimo = min(EXPORTACIONES, na.rm = TRUE),
Maximo = max(EXPORTACIONES, na.rm = TRUE)
)
# Mostrar el resumen
print(resumen_2023)
library(ggplot2)
# Gráfico de barras para 2023
ggplot(exportaciones_2023, aes(x = PAIS, y = EXPORTACIONES, fill = PAIS)) +
geom_bar(stat = "identity") +
theme_minimal() +
labs(title = "Exportaciones FOB por País en 2023", y = "Exportaciones (USD)", x = "País")
# Gráfico de líneas para cada año
exportaciones_fob_long <- exportaciones_fob %>%
pivot_longer(cols = `2016`:`2023`, names_to = "AÑO", values_to = "EXPORTACIONES")
# Gráfico de densidad por país
ggplot(exportaciones_2023, aes(x = EXPORTACIONES, fill = PAIS)) +
geom_density(alpha = 0.5) +
theme_minimal() +
labs(title = "Densidad de Exportaciones por País en 2023",
y = "Densidad", x = "Exportaciones (USD)")
# Filtrar datos de 2016 y 2023 para países asiáticos
asiaticos_2016 <- exportaciones_fob %>% filter(PAIS %in% c("CHINA", "JAPON")) %>%
select(PAIS, `2016`) %>% rename(EXPORTACIONES = `2016`)
asiaticos_2023 <- exportaciones_fob %>% filter(PAIS %in% c("CHINA", "JAPON")) %>%
select(PAIS, `2023`) %>% rename(EXPORTACIONES = `2023`)
# Prueba t entre 2016 y 2023 para países asiáticos
if (nrow(asiaticos_2016) >= 2 & nrow(asiaticos_2023) >= 2) {
resultado_asiaticos <- t.test(asiaticos_2016$EXPORTACIONES, asiaticos_2023$EXPORTACIONES)
print(resultado_asiaticos)
} else {
cat("No hay suficientes datos para la prueba t en países asiáticos.\n")
}
# Prueba de varianza entre México y China en 2023
var_test <- var.test(mexico_2023$EXPORTACIONES, china_2023$EXPORTACIONES)
print(var_test)
setwd("~/Documents/GitHub/Data-Wrangling/Laboratorio #7")
library(readr)
Health_and_Personal_Care_metadata <- read_csv("Health_and_Personal_Care_metadata.csv")
View(Health_and_Personal_Care_metadata)
Health_and_Personal_Care <- read_csv("Health_and_Personal_Care.csv")
View(Health_and_Personal_Care)
library(readr)
Health_and_Personal_Care_metadata <- read_csv("Health_and_Personal_Care_metadata.csv")
View(Health_and_Personal_Care_metadata)
Health_and_Personal_Care <- read_csv("Health_and_Personal_Care.csv")
View(Health_and_Personal_Care)
# Filtrar las reseñas que contienen las palabras clave
filtered_reviews <- Health_and_Personal_Care[
grepl("love", Health_and_Personal_Care$text, ignore.case = TRUE) &
grepl("recommend", Health_and_Personal_Care$text, ignore.case = TRUE) &
grepl("enjoy", Health_and_Personal_Care$text, ignore.case = TRUE),
]
# Contar productos únicos con esas reseñas
unique_products <- length(unique(filtered_reviews$product_id))
# Mostrar el resultado
print(unique_products)
# Unir las reviews filtradas con los datos de metadata para obtener las tiendas
merged_data <- inner_join(filtered_reviews,
Health_and_Personal_Care_metadata,
by = c("product_id" = "parent_id"))
library(dplyr)
# Unir las reviews filtradas con los datos de metadata para obtener las tiendas
merged_data <- inner_join(filtered_reviews,
Health_and_Personal_Care_metadata,
by = c("product_id" = "parent_id"))
# Contar el número de reseñas por tienda
store_counts <- merged_data %>%
group_by(store) %>%
summarise(review_count = n()) %>%
arrange(desc(review_count))
# Obtener el top 5 de las tiendas con más reseñas
top_5_stores <- head(store_counts, 5)
# Mostrar el resultado
print(top_5_stores)
library(tm)
install.packages("tm")
library(tm)
library(wordcloud)
install.packages("wordcloud")
library(tm)
library(wordcloud)
# Combinar todo el texto de las reseñas filtradas
all_text <- paste(filtered_reviews$text, collapse = " ")
# Crear un corpus y limpiar los datos
corpus <- Corpus(VectorSource(all_text))
corpus <- tm_map(corpus, content_transformer(tolower)) # Convertir a minúsculas
corpus <- tm_map(corpus, removePunctuation)            # Eliminar puntuación
corpus <- tm_map(corpus, removeNumbers)                # Eliminar números
corpus <- tm_map(corpus, removeWords, stopwords("en")) # Eliminar stopwords en inglés
# Generar el wordcloud
wordcloud(corpus, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
library(RColorBrewer)
# Filtrar las reseñas correspondientes a las 5 tiendas
store_reviews <- merged_data %>%
filter(store %in% top_5_stores$store)
# Combinar todo el texto de las reseñas filtradas
all_text <- paste(store_reviews$text, collapse = " ")
# Crear un corpus y limpiar los datos
corpus <- Corpus(VectorSource(all_text))
corpus <- tm_map(corpus, content_transformer(tolower)) # Convertir a minúsculas
corpus <- tm_map(corpus, removePunctuation)            # Eliminar puntuación
corpus <- tm_map(corpus, removeNumbers)                # Eliminar números
corpus <- tm_map(corpus, removeWords, stopwords("en")) # Eliminar stopwords en inglés
# Generar el wordcloud
wordcloud(corpus, max.words = 100, random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# Crear un TermDocumentMatrix
tdm <- TermDocumentMatrix(corpus)
matrix <- as.matrix(tdm)
# Calcular la frecuencia de cada palabra
word_freq <- sort(rowSums(matrix), decreasing = TRUE)
# Obtener las 25 palabras más frecuentes
top_25_words <- head(word_freq, 25)
# Mostrar el resultado
print(top_25_words)
suppressPackageStartupMessages({
library(readr)
library(dplyr)       # Para manipulación de datos
library(tm)          # Para procesamiento de texto
library(wordcloud)   # Para generar nubes de palabras
library(RColorBrewer) # Para paletas de colores en las nubes de palabras
})
Health_and_Personal_Care_metadata <- read_csv("Health_and_Personal_Care_metadata.csv")
View(Health_and_Personal_Care_metadata)
Health_and_Personal_Care <- read_csv("Health_and_Personal_Care.csv")
View(Health_and_Personal_Care)
